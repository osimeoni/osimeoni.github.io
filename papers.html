<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Oriane Siméoni</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Add icon library -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" >

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Josefin+Slab:100,300,400,600,700,100italic,300italic,400italic,600italic,700italic" rel="stylesheet" type="text/css">

    <!-- Custom styles for this template -->
    <link href="css/business-casual.css" rel="stylesheet">

  </head>

  <body>

    <div class="tagline-upper text-center text-heading text-shadow text-white mt-5 d-none d-lg-block"></div>

    <!-- Navigation -->
    <nav class="navbar sticky-top navbar-expand-lg navbar-static navbar-light bg-faded py-lg-4">
      <div class="container">
        <div class="text navbar-text">
          <h1> Oriane Siméoni </h1>
        </div>
        <button class=  "navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav mx-auto">
            <li class="nav-item active px-lg-5">
              <a class="nav-link text-uppercase text-expanded" href="index.html">Home
              </a>
            </li>
            <li class="nav-item px-lg-5">
              <a class="nav-link text-uppercase text-expanded" href="papers.html">Papers</a>
                <span class="sr-only">(current)</span>
            </li>
            <li class="nav-item px-lg-5">
              <a class="nav-link text-uppercase text-expanded" href="cv.pdf">Resume</a>
            </li>
            <li class="nav-item px-lg-5">
              <a class="nav-link text-uppercase text-expanded" href="index.html#contact">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

	
	<div class="container">

    <div class="bg-faded p-4 my-4">
      <hr class="divider">
      <h3 class="text-center text-lg text-uppercase my-0">
         Localizing Objects with Self-Supervised Transformers and no Labels
      </h3>
      <h4 class="text-center text-lg my-0">
         <i>O. Siméoni, G. Puy, H. V. Vo, S. Roburin, S. Gidaris, A. Bursuc, P. Pérez, R. Marlet and J. Ponce</i>, arxiv 2021
      </h4>
      <hr class="divider">
      <div class="row">
        <div class="col-lg-3" style="margin-top: 1.7cm;">
          <!-- <a href="https://arxiv.org/abs/1709.04725" > -->
          <img class="img-fluid mb-4 mb-lg-0" src="img/lost_2021.png" align="middle" alt="" style=" width: 100% ; "/>
          <!-- </a> -->
        </div>
        <div class="col-lg-9">
        Localizing objects in image collections without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task. The code to reproduce our results can be found at https://github.com/valeoai/LOST. 
      <p> 
        <a href="https://arxiv.org/pdf/2109.14279.pdf"> [pdf] </a>
        <a href="https://github.com/valeoai/LOST"> [code] </a>
      </p>
      </div>
      </div>
    </div>

    <div class="bg-faded p-4 my-4">
      <hr class="divider">
      <h3 class="text-center text-lg text-uppercase my-0">
         Robust image representation for classification, retrieval and object discovery
      </h3>
      <h4 class="text-center text-lg my-0">
         <i>O. Siméoni</i>, PhD thesis
      </h4>
      <hr class="divider">
      <div class="row">
        <div class="col-lg-3" style="margin-top: 1.7cm;">
          <!-- <a href="https://arxiv.org/abs/1709.04725" > -->
          <img class="img-fluid mb-4 mb-lg-0" src="img/simeoni_2020.png" align="middle" alt="" style=" width: 100% ; height: 70%"/>
          <!-- </a> -->
        </div>
        <div class="col-lg-9">

          Neural network representations proved to be relevant for many computer vision tasks such as image classification, object detection, segmentation or instance-level image retrieval. A network is trained for one particular task and requires a large number of labeled data. We propose in this thesis solutions to extract the most information with the least supervision. First focusing on the classification task, we examine the active learning process in the context of deep learning and show that combining it to semi-supervised and unsupervised techniques boost greatly results. We then investigate the image retrieval task, and in particular we exploit the spatial localization informa- tion available “for free” in CNN feature maps. We first propose to represent an image by a collection of affine local features detected within activation maps, which are memory-efficient and robust enough to perform spatial matching. Then again extracting information from feature maps, we discover objects of interest in images of a dataset and gather their representations in a nearest neighbor graph. Using the centrality measure on the graph, we are able to construct a saliency map per image which focuses on the repeating objects and allows us to compute a global representation excluding clutter and background.
      <p> 
        <a href="https://hal.inria.fr/tel-03082952/document"> [pdf] </a>
        <a> [slides] </a>
      </p>
      </div>
      </div>
    </div>

    <div class="bg-faded p-4 my-4">
        <hr class="divider">
        <h3 class="text-center text-lg text-uppercase my-0">
           Rethinking deep active learning: Using unlabeled data at model training
        </h3>
        <h4 class="text-center text-lg my-0">
           <i>O. Siméoni, M. Budnik, Y. Avrithis and G. Gravier</i>, ICPR 2020
        </h4>
        <hr class="divider">
        <div class="row">
          <div class="col-lg-3">
            <!-- <a href="https://arxiv.org/abs/1709.04725" > -->
            <img class="img-fluid mb-4 mb-lg-0" src="img/active.svg" align="middle" alt="" style=" width: 100% ; height: 80%"/>
            <!-- </a> -->
          </div>
          <div class="col-lg-9">

            Active learning typically focuses on training a model on few labeled examples alone, while unlabeled ones are only used for acquisition. In this work we depart from this setting by using both labeled and unlabeled data during model training across active learning cycles. We do so by using unsupervised feature learning at the beginning of the active learning pipeline and semi-supervised learning at every active learning cycle, on all available data. The former has not been investigated before in active learning, while the study of latter in the context of deep learning is scarce and recent findings are not conclusive with respect to its benefit. Our idea is orthogonal to acquisition strategies by using more data, much like ensemble methods use more models. By systematically evaluating on a number of popular acquisition strategies and datasets, we find that the use of unlabeled data during model training brings a surprising accuracy improvement in image classification, compared to the differences between acquisition strategies. We thus explore smaller label budgets, even one label per class.

        <p> 
        <a href="https://arxiv.org/pdf/1911.08177.pdf"> [pdf] </a> 
        <a href="https://github.com/osimeoni/RethinkingDeepActiveLearning"> [code] </a> 
        </p>
        </div>
        </div>
      </div>
      
      
    <div class="bg-faded p-4 my-4">
        <hr class="divider">
        <h3 class="text-center text-lg text-uppercase my-0">
            Local Features and Visual Words Emerge in Activations
        </h3>
        <h4 class="text-center text-lg my-0">
            <i>O. Siméoni, Y. Avrithis and O. Chum</i>, CVPR 2019
        </h4>
        <hr class="divider">
        <div class="row">
          <div class="col-lg-3" style="margin-top: 0.6cm;">
            <img class="img-fluid mb-4 mb-lg-0" src="img/cvpr2019.jpg" align="middle" alt="" style=" width: 100% ; height: 80%"/>
            <!-- </a> -->
          </div>
          <div class="col-lg-9">

            We propose a novel method of spatial verification for image retrieval. Initial ranking is based on image descriptors extracted from convolutional neural network activations by global pooling, as in recent state-of-the-art work. However, the same sparse 3D activation tensor is also approximated by a collection of local features. These local features are then robustly matched to approximate the optimal alignment of the tensors. This happens without any network modification, additional layers or training. No local feature detection happens on the original image; no local feature descriptors and no visual vocabulary are needed throughout the whole process.

            We experimentally show that the proposed method achieves the state-of-the-art performance on standard benchmarks across different network architectures and different global pooling methods. Advantages of combining efficient nearest neighbor retrieval with global descriptors and spatial verification is even more pronounced by spatially verified diffusion.

        <p> 
        <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Simeoni_Local_Features_and_Visual_Words_Emerge_in_Activations_CVPR_2019_paper.pdf"> [pdf] </a> 
        <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Simeoni_Local_Features_and_Visual_Words_Emerge_in_Activations_CVPR_2019_paper.html"> [bibtex] </a> 
        <a href="https://github.com/osimeoni/DSM"> [code] </a> 
        </p>
        </div>
        </div>
      </div>

      <div class="bg-faded p-4 my-4">
        <hr class="divider">
        <h3 class="text-center text-lg text-uppercase my-0">
          Graph-based particular object discovery
        </h3>
        <h4 class="text-center text-lg my-0">
            <i>O. Siméoni, A. Iscen, G. Tolias, Y. Avrithis and O. Chum</i>, MVA 2019
        </h4>
        <hr class="divider">
        <div class="row">
          <div class="col-lg-3">
            <a href="https://dblp.org/rec/bibtex/journals/mva/SimeoniITAC19" >
              <img class="img-fluid mb-4 mb-lg-0" src="img/wacv2017.png" align="middle" alt="" style=" width: 85% ; height: 85%"/>
            </a>
          </div>
          <div class="col-lg-9">
        <p align="justify">
         Severe background clutter is challenging in many computer vision tasks, including large-scale image retrieval. Global descriptors, that are popular due to their memory and search efficiency, are especially prone to corruption by such a clutter. Eliminating the impact of the clutter on the image descriptor increases the chance of retrieving relevant images and prevents topic drift due to actually retrieving the clutter in the case of query expansion. In this work, we propose a novel salient region detection method. It captures, in an unsupervised manner, patterns that are both discriminative and common in the dataset. Saliency is based on a centrality measure of a nearest neighbor graph constructed from regional CNN representations of dataset images. The proposed method exploits recent CNN architectures trained for object retrieval to construct the image representation from the salient regions. We improve particular object retrieval on challenging datasets containing small objects. 

        </p> 
        <p> 
        <a href="https://avrithis.net/data/pub/pdf/journ/J28.mva18.disco.pdf"> [pdf] </a>
        <a href="https://dblp.org/rec/bibtex/journals/mva/SimeoniITAC19"> [bibtex] </a>
        </p>
        </div>
        </div>
      </div>

      <div class="bg-faded p-4 my-4">
        <hr class="divider">
        <h3 class="text-center text-lg text-uppercase my-0">
          Unsupervised object discovery for instance recognition
        </h3>
        <h4 class="text-center text-lg my-0">
             <i>O. Siméoni, A. Iscen, G. Tolias, Y. Avrithis and O. Chum</i>, WACV 2018 
        </h4>
        <hr class="divider">
        <div class="row">
          <div class="col-lg-3">
            <a href="https://arxiv.org/abs/1709.04725" >
            	<img class="img-fluid mb-4 mb-lg-0" src="img/wacv2017.png" align="middle" alt="" style=" width: 80% ; height: 80%"/>
            </a>
          </div>
          <div class="col-lg-9">
            <p align="justify">
            Severe background clutter is challenging in many computer
				vision tasks, including large-scale image retrieval.
				Global descriptors, that are popular due to their memory
				and search efficiency, are especially prone to corruption
				by such clutter. Eliminating the impact of the clutter on
				the image descriptor increases the chance of retrieving relevant
				images as well as preventing topic drift by actually
				retrieving the clutter in the case of query expansion. 
			</p>
			<p align="justify"> In this
				work, we propose a novel salient region detection method.
				It captures, in an unsupervised manner, patterns that are
				both discriminative and common in the dataset. The descriptors
				derived on the salient regions improve particular
				object retrieval, most noticeably in a large collections containing
				small objects.
       	</p>
        <p> 
        <a href="https://arxiv.org/abs/1709.04725"> [pdf] </a>
        <a href="https://dblp.org/rec/bibtex/conf/wacv/SimeoniITAC18"> [bibtex] </a>
        <a href="https://www.youtube.com/watch?v=BkmOHwKUCTA"> [video] </a>
        </p>
        </div>
        </div>
      </div>

      <div class="bg-faded p-4 my-4">
        <hr class="divider">
        <h3 class="text-center text-lg text-uppercase my-0">
          Tracking global gene expression responses in T cell differentiation
        </h3>
        <h4 class="text-center text-lg my-0">
             <i>O. Siméoni, V. Piras, M. Tomita and K. Selvarajoo</i>, GENE 2015
        </h4>
        <hr class="divider">
        <div class="row">
          <div class="col-lg-3">
            <a href="http://www.sciencedirect.com/science/article/pii/S0378111915006460" >
              <img class="img-fluid mb-4 mb-lg-0" src="img/genes2015.jpg" align="middle" alt="" style=" width: 80% ; height: 80%"/>
            </a>
          </div>
          <div class="col-lg-9">
          <p align="justify">Upon receiving antigens from the innate immune cells, CD4+ T cells differentiate into distinct effector cells. To probe the global responses of distinct effector cells, we analyzed transcriptome-wide expressions of Th1, Th2, Treg and Th17 using Pearson correlation, entropy and principal component analyses, with Th0 as a control. Although the global response of Th0 was quite distinct from Th17, surprisingly, it was highly similar to Th1, Th2 and Treg. Moreover, 8 major temporal groups consisting of 5704 differentially expressed genes were revealed for both Th0 and Th17. Gene functional enrichment analysis showed immune responses and metabolic processes were mainly activated between Th0 and Th17, while genes related to cell cycle and replication were differentially regulated. Moreover, we found the upregulation of several novel genes for Th0 and Th17. Overall, we deduce that Th0 is globally similar to Th1, Th2 and Treg. Our results indicate that Th0 is a differentiated state and, therefore, may not be used as a control cell type.
          </p>
        <p> 
        <a href="https://ac.els-cdn.com/S0378111915006460/1-s2.0-S0378111915006460-main.pdf?_tid=3dac5152-d531-11e7-b3a8-00000aacb35f&acdnat=1511979492_936f55347116c114ce1d067f9df03cf2"> [pdf] </a>
        </p>
        </div>
        </div>
      </div>

    </div>
    <!-- /.container -->

    <!--
    <footer class="bg-faded text-center py-5">
      <div class="container">
        <p class="m-0">Copyright &copy; Your Website 2017</p>
      </div>
    </footer>
    <-->

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  </body>

</html>
